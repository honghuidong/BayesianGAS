---
title: "Model Comparisons: Dynamic Pooled Marked Point Process Models"
output: 
  rmarkdown::html_vignette:
vignette: >
  %\VignetteIndexEntry{Model Comparisons: Dynamic Pooled Marked Point Process Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{bridgesampling, coda}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.show = 'hold',
  fig.width = 8,
  fig.height = 7,
  fig.align = 'center'
)
```

# Model Comparisons: Dynamic Pooled Marked Point Process Models
The dataset used in the Dynamic Pooled Marked Point Process (DPMP) models application of section 4.2 is not public. The analysis presented here is equivalent but based on simulated data. As a result some of the Bayesian model comparison results are different from the results reported in the thesis. Also, some of the plots might no longer convey the same insights.

```{r lib}
library(BayesianGAS)
```

```{r seed_and_constants}
set.seed(100)
kTransitionTypes <- c("IGtoSIG", "IGtoD", "SIGtoIG", "SIGtoD")
kScalings <- c(0., -0.5, -1.0)
kNumFactorSpecs <- c(1, 2, 3)
kNumParamsVec <- c(9, 10, 12)
kNumTransitions <- 4
```

## Simulate Data 

The number of firms and the number of events used in simulation is similar to that of the real data set. Also, events are aggregated per month to reflect how the real data set is formatted. The DPMP model chosen for simulation using a 3 factor specification and inverse FI scaling as these choices seemed to fit the real data set best.

```{r sim_data}
numInitIG  <- 750
numInitSIG <- 1000
numEvents  <- 2500
numFactors <- 3
scaling    <- -1.
simParams <- c(
  A = c(0.055, 0.04, 0.06),
  B = c(0.99, 0.96, 0.98),
  C = c(0.9, 1.1),
  W = c(-5.48, -10.13, -5.46, -5.76)
)
dpmpSimModel <- new(DPMP, simParams, numFactors, scaling)
sims <- dpmpSimModel$Simulate(numInitIG, numInitSIG, numEvents, rep(0, numFactors))
tauRaw <- as.integer(cumsum(sims[,5]))
transitionsRaw <- data.frame(sims[,c(1:4)])
possibleTranstionsRaw <- data.frame(sims[,c(6:9)])
colnames(possibleTranstionsRaw) <- kTransitionTypes
colnames(transitionsRaw) <- kTransitionTypes
transitions <- as.matrix(aggregate(transitionsRaw, by = list(tauRaw), FUN = sum))
tau <- transitions[, 'Group.1']
diffTau <- diff(tau)
transitions <- transitions[-1, kTransitionTypes]
endTau <- tail(tau, n = 1)
dates <- seq(as.Date("1986-02-01"), by = "month", length.out = endTau)[tau] - 1
possibleTranstions <- as.matrix(
  aggregate(possibleTranstionsRaw, by = list(tauRaw), 
            FUN = function(x){tail(x, n = 1)}))[-1, kTransitionTypes]
y <- cbind(transitions, diffTau, possibleTranstions)
```

### Plot Transitions
```{r, fig.cap = "Transitions"}
par(mfcol = c(2, 2))
for (tt in kTransitionTypes) {
  leg <- paste(tt, "counts")
  plot(dates, transitions[,tt], type = "l", xaxt = "n", xlab = "Years", 
       ylab = "Intensity")
  legend("topleft", legend = leg, bty = "n", lty = 1)
  axis.Date(1, at = seq(min(dates), max(dates), by = "1 years"), format = "%Y")
}
```
### Set data attributes
```{r set_data_attr}
numObs <- dim(y)[1]
```

## Maximum Likelihood (ML) estimation 

Looking at the ML estimates for the DPMP3-Inv model in comparison to the true parameters used to simulate the data, it is evident that it is challenging to recover the true parameters of the data generating process given the limited number of points in time upon which events are recorded (~400 in this case). 

```{r ML_estimation}
initParamVecs <- list(
  c(A = 0.01, B = 0.9, C = c(0.5, 0.5, -0.5) , w = c(-5, -10, -5, -5)),
  c(A = rep(0.01, 2), B = rep(0.9, 2), C = rep(0.1, 2) , w = c(-5, -10, -5, -5)),
  c(A = rep(0.01, 3), B = rep(0.9, 3), C = rep(0.1, 2) , w = c(-5, -10, -5, -5))
)
parScaleVecs <- list(
  c(A = 0.01, B = 0.01, C = rep(0.05, 3) , w = rep(0.2, 4)),
  c(A = rep(0.01, 2), B = rep(0.02, 2), C = rep(0.05, 2) , w = rep(0.2, 4)),
  c(A = rep(0.01, 3), B = rep(0.01, 3), C = rep(0.05, 2) , w = rep(0.2, 4))
)
lowerBoundVecs <- list(
  c(A = -Inf, B = -1, C = rep(-Inf, 3) , w = rep(-Inf, 4)),
  c(A = rep(-Inf, 2), B = rep(-1, 2), C = rep(-Inf, 2), w = rep(-Inf, 4)),
  c(A = rep(-Inf, 3), B = rep(-1, 3), C = rep(-Inf, 2), w = rep(-Inf, 4))
)
upperBoundVecs <- list(
  c(A = Inf, B = 0.99, C = rep(Inf, 3) , w = rep(Inf, 4)),
  c(A = rep(Inf, 2), B = rep(0.999, 2), C = rep(Inf, 2), w = rep(Inf, 4)),
  c(A = rep(Inf, 3), B = rep(0.999, 3), C = rep(Inf, 2), w = rep(Inf, 4))
)
f1s <- list(0, rep(0, 2), rep(0, 3))
modelsML <- list()
for (i in 1:3) {
  numF <- kNumFactorSpecs[i]
  initParams <- initParamVecs[[i]]
  parScales <- parScaleVecs[[i]]
  lb <- lowerBoundVecs[[i]]
  ub <- upperBoundVecs[[i]]
  f1 <- f1s[[i]]
  for (s in kScalings) {
    dpmp <- new(DPMP, numF, s)
    cat("Fitting model: ", dpmp$Name, sprintf("\n"))
    dpmp <- FitML(
      model = dpmp,
      initParams = initParams,
      y = y,
      f1 = f1,
      method = "L-BFGS-B",
      control = list(
        maxit = 1e5, 
        parscale = parScales
      ),
      hessian = TRUE,
      verbose = TRUE,
      lower = lb,
      upper = ub
    )
    modelsML <- c(modelsML, dpmp)
    names(modelsML) <- c(names(modelsML)[1:length(modelsML) - 1], dpmp$Name)
    cat(sprintf("\n"))
  }
}
```

## MCMC using RWMH

I deviate slightly here from the analysis presented in the thesis, by thinning the posterior sample by a factor of 10 (i.e. I keep only 1 out of 10 draws). This is done to reduce memory usage.

```{r set_iters_and_priors}
iter <- 4e5
thinning <- 10
numDraws <- floor(iter / thinning)
warmUpRounds <- c(3, 5, 6)
priorStacks <- list(
  new(
    PriorStack,
    c("Normal", "TruncatedNormal", rep("Normal", 7)),
    list(
      c(0.05, 1),
      c(0.95, 1, -1, 1),
      c(0.5, 5),
      c(0.5, 5),
      c(-0.5, 5),
      c(-5, 5),
      c(-10, 5),
      c(-5, 5),
      c(-5, 5)
    )
  ),
  new(
    PriorStack,
    c(rep("Normal", 2), rep("TruncatedNormal", 2), rep("Normal", 6)),
    list(
      c(0.05, 1),
      c(0.05, 1),
      c(0.95, 1, -1, 1),
      c(0.95, 1, -1, 1),
      c(0.5, 5),
      c(0.5, 5),
      c(-5, 5),
      c(-10, 5),
      c(-5, 5),
      c(-5, 5)
    )
  ),
  new(
    PriorStack,
    c(rep("Normal", 3), rep("TruncatedNormal", 3), rep("Normal", 6)),
    list(
      c(0.05, 1),
      c(0.05, 1),
      c(0.05, 1),
      c(0.95, 1, -1, 1),
      c(0.95, 1, -1, 1),
      c(0.95, 1, -1, 1),
      c(0.5, 5),
      c(0.5, 5),
      c(-5, 5),
      c(-10, 5),
      c(-5, 5),
      c(-5, 5)
    )
  )
)
```

### Run RWMH
```{r run_RWMH, warning = FALSE, collapse = FALSE}
drawsRWMHLst <- list()
for (i in 1:3) {
  numF <- kNumFactorSpecs[i]
  initParams <- initParamVecs[[i]]
  f1 <- f1s[[i]]
  priorStack <- priorStacks[[i]]
  numParams <- kNumParamsVec[i]
  for (s in kScalings) {
    if ((s  == -1) && (numF > 1)) {
      stepsize1 <- 0.0025
    }else{
      stepsize1 <- 0.006
    }
    dpmp <- new(DPMP, numF, s)
    cat("Running RWMH for model: ", dpmp$Name, sprintf("\n"))
    startTime <- Sys.time()
    cat(sprintf("Warm up 1 \n"))
    warmUpRWMH <- RWMH(
      dpmp$Name,
      priorStack,
      y = y,
      f1 = f1,
      initParams = initParams,
      sigma = diag(numParams),
      iter = 1e4,
      stepsize = 0.006,  # stepsize1,
      printIter = 1e5,
      thinning = thinning
    )
    for (round in 2:max(2, warmUpRounds[i])) {
      cat(sprintf("Warm up %i \n", round))
      warmUpRWMH <- RWMH(
        dpmp$Name,
        priorStack,
        y = y,
        f1 = f1,
        initParams = initParams,
        sigma = cov(warmUpRWMH),
        iter = 2e4,
        stepsize = .2,
        printIter = 1e5,
        thinning = thinning
      )
    }
    drawsRWMH <- RWMH(
      dpmp$Name,
      priorStack,
      y = y,
      f1 = f1,
      initParams = initParams,
      sigma = cov(warmUpRWMH),
      iter = iter,
      stepsize = .4,
      printIter = 1e5,
      thinning = thinning
    )
    endTime <- Sys.time()
    timeRWMH <- difftime(endTime, startTime, units = 'secs')
    cat("RWMH Time: ", timeRWMH, sprintf(" seconds\n"))
    colnames(drawsRWMH) <- names(initParams)
    drawsRWMHLst <- append(drawsRWMHLst, list(drawsRWMH))
    names(drawsRWMHLst) <- 
      c(names(drawsRWMHLst)[1:length(drawsRWMHLst) - 1], dpmp$Name)
    
    ESSs <- coda::effectiveSize(drawsRWMH)
    ESSs <- t(data.frame(ESSs))
    colnames(ESSs) <- names(initParams)
    print(round(ESSs, 1))
    cat(sprintf("\n"))
  }
}
```
## Model comparisons


```{r}
IC <- function(logl, npar, k = log(npar)){
  IC <- -2 * logl + k * npar
  return(IC)
}
modelScores <- data.frame(matrix(
  0, 
  nrow = 9, 
  ncol = 3, 
  dimnames = list(names(modelsML), c("MarginalLikelihood", "LogLikelihood", "BIC"))
))
burn <- 1000
```

### Compute marginals and Bayesian Information criteria (BICs)
```{r, error = TRUE, results = 'hide'}
upperBoundVecs <- list(
  c(A = Inf, B = 1., C = rep(Inf, 3) , w = rep(Inf, 4)),
  c(A = rep(Inf, 2), B = rep(1., 2), C = rep(Inf, 2), w = rep(Inf, 4)),
  c(A = rep(Inf, 3), B = rep(1., 3), C = rep(Inf, 2), w = rep(Inf, 4))
)
marginalsLst <- list()
logLikList <- list()
for (i in 1:3) {
  numF <- kNumFactorSpecs[i]
  initParams <- initParamVecs[[i]]
  f1 <- f1s[[i]]
  priorStack <- priorStacks[[i]]
  numParams <- kNumParamsVec[i]
  lb <- lowerBoundVecs[[i]]
  ub <- upperBoundVecs[[i]]
  for (s in kScalings) {
    dpmp <- new(DPMP, initParams, priorStack, numF, s)
    logPosterior <- function(pars, data, printErrors = FALSE) {
      out <- tryCatch(
        {dpmp$LogPosteriorWPar(pars, y, f1)},
        error = function(cond) {
          if (printErrors) message(cond)
          return(-Inf)
        }
      )
      return(out)
    }
    cat("Computing marginal for model: ", dpmp$Name, sprintf("\n"))
    marginal <- bridgesampling::bridge_sampler(
      drawsRWMHLst[[dpmp$Name]][-(1:burn), ], 
      log_posterior = logPosterior,
      data = NULL,
      printErrors = FALSE,
      method = "warp3",
      lb = lb,
      ub = ub
    )
    modelScores[dpmp$Name, "MarginalLikelihood"] <- marginal$logml
    modelScores[dpmp$Name, "LogLikelihood"] <- modelsML[[dpmp$Name]]$LogLValML
    modelScores[dpmp$Name, "BIC"] <- IC(modelsML[[dpmp$Name]]$LogLValML, numParams)
  }
}
```

```{r, echo = FALSE}
knitr::kable(modelScores, caption = "Marginals, log-likelihoods and BICS")
```

### Compute Bayes Factors (BFs)
```{r}
marginals <- modelScores["MarginalLikelihood"]
bf1H1I <- bridgesampling::bayes_factor(
  marginals["DPMP1-H", ], marginals["DPMP1-I", ], TRUE)
bf1Inv1H <- bridgesampling::bayes_factor(
  marginals["DPMP1-Inv", ], marginals["DPMP1-H", ], TRUE)
bf2H2I <- bridgesampling::bayes_factor(
  marginals["DPMP2-H", ], marginals["DPMP2-I", ], TRUE)
bf2Inv2H <- bridgesampling::bayes_factor(
  marginals["DPMP2-Inv", ], marginals["DPMP2-H", ], TRUE)
bf3H3I <- bridgesampling::bayes_factor(
  marginals["DPMP3-H", ], marginals["DPMP3-I", ], TRUE)
bf3Inv3H <- bridgesampling::bayes_factor(
  marginals["DPMP3-Inv", ], marginals["DPMP3-H", ], TRUE)
bf2I1I <- bridgesampling::bayes_factor(
  marginals["DPMP2-I", ], marginals["DPMP1-I", ], TRUE)
bf2H1H <- bridgesampling::bayes_factor(
  marginals["DPMP2-H", ], marginals["DPMP1-H", ], TRUE)
bf2Inv1Inv <- bridgesampling::bayes_factor(
  marginals["DPMP2-Inv", ], marginals["DPMP1-Inv", ], TRUE)
bf3I1I <- bridgesampling::bayes_factor(
  marginals["DPMP3-I", ], marginals["DPMP1-I", ], TRUE)
bf3H1H <- bridgesampling::bayes_factor(
  marginals["DPMP3-H", ], marginals["DPMP1-H", ], TRUE)
bf3Inv1Inv <- bridgesampling::bayes_factor(
  marginals["DPMP3-Inv", ], marginals["DPMP1-Inv", ], TRUE)
```

1-H | 1-I : `r bf1H1I$bf`  
1-Inv | 1-H : `r bf1Inv1H$bf`  
2-H | 2-I : `r bf2H2I$bf`  
2-Inv | 2-H : `r bf2Inv2H$bf`  
3-H | 3-I : `r bf3H3I$bf`  
3-Inv | 3-H : `r bf3Inv3H$bf`  
2-I | 1-I : `r bf2I1I$bf`  
2-H | 1-H : `r bf2H1H$bf`  
2-Inv | 1-Inv : `r bf2Inv1Inv$bf`  
3-I | 1-I : `r bf3I1I$bf`  
3-H | 1-H : `r bf3H1H$bf`  
3-Inv | 1-Inv : `r bf3Inv1Inv$bf`  

## Some Parameter Statistics
```{r, results = "asis"}
selectedModels <- c("DPMP1-Inv", "DPMP2-Inv", "DPMP3-Inv")
for (model in selectedModels) {
  summary_ <- summary(coda::mcmc(drawsRWMHLst[[model]]))$statistics
  print(knitr::kable(summary_, caption = model))
}
```

## Plots

### Posterior of intensity
```{r}
intensityDraws <- 
  array(0, dim = c(numDraws - burn, numObs, kNumTransitions))
meanLogIntensitiesLst <- list()
for (i in c(1, 3)) {
  numF <- kNumFactorSpecs[i]
  f1 <- f1s[[i]]
  for (s in kScalings[c(1,3)]) {
    dpmp <- new(DPMP, numF, s)
    for (i in 1:(numDraws - burn)) {
      dpmp$SetParams(as.vector(drawsRWMHLst[[dpmp$Name]][burn + i,]))
      intensityDraws[i, , ] <- dpmp$IntensityFilter(y, f1, TRUE)
      if ((i > 0) && ((i %% 1e4) == 0)) {
        cat(sprintf("iter %i\n", i));
      }
    }
    meanLogIntensities <- colMeans(intensityDraws)
    colnames(meanLogIntensities) <- kTransitionTypes
    meanLogIntensitiesLst <- 
      append(meanLogIntensitiesLst, list(meanLogIntensities))
    names(meanLogIntensitiesLst) <- c(
      names(meanLogIntensitiesLst)[1:length(meanLogIntensitiesLst) - 1], dpmp$Name)
  }
}
intensityDraws <- exp(intensityDraws)
```

### Mean log intensity plots
```{r, fig.height = 8, fig.width = 9, fig.cap = "Mean Log Intensities"}
selectedModels <- c("DPMP1-I", "DPMP3-I", "DPMP1-Inv", "DPMP3-Inv")
labY <- c(
  expression("Log intensity" ~ IG %->% SIG), 
  expression("Log intensity" ~ IG %->% D), 
  expression("Log intensity" ~ SIG %->% IG), 
  expression("Log intensity" ~ SIG %->% D)
)
names(labY) <- kTransitionTypes
legMeans <- c(
  "1 factor Identity scaling",
  "3 factor Identity scaling",
  "1 factor Inverse FI scaling",
  "3 factor Inverse FI scaling"
)
names(legMeans) <- selectedModels
yLims <- list(c(-8, -3.5), c(-14, -7), c(-6.3, -4.5), c(-9, -3.5))
names(yLims) <- kTransitionTypes
par(mfcol = c(2, 2))
for (tt in kTransitionTypes) {
  for (model in selectedModels) {
    if (grepl("3", model)) lty = 3 else lty = 1
    if (grepl("Inv", model)) col = "blue" else col = "red"
    if (model == selectedModels[1]) {
      plot(dates, meanLogIntensitiesLst[[model]][,tt], type = "l", xaxt = "n", 
           xlab = "Years",  ylab = labY[tt], lty = lty, ylim = yLims[[tt]],
           col = col)
    }else{
      lines(dates, meanLogIntensitiesLst[[model]][,tt], lty = lty, col = col)
    }
    axis.Date(1, at = seq(min(dates), max(dates), by = "1 years"), format = "%Y")
  }
  legend("topleft", bty = "n", col = c("red", "red", "blue", "blue"), 
         lty=c(1, 3, 1, 3), legend = legMeans)
}
```


### Joint distribution plots
```{r, fig.height = 8, fig.width = 5.5}
selectedDraws <- drawsRWMHLst[["DPMP3-Inv"]][-(1:burn), ]
par(mfrow = c(2, 1), mar = c(4.2, 4.2, 1, 2))
plot(
  selectedDraws[, "B2"],
  selectedDraws[, "A2"],
  cex = 0.2,
  cex.axis = 1,
  xlab = expression(b[2]),
  ylab = expression(a[2])
)
abline(h = coda::HPDinterval(coda::mcmc(selectedDraws[, "A2"])), col = "red", 
       lty = 2)
leg <- expression("95% HPD bounds for" ~ a[2])
legend("topleft", legend = leg, col = "red", lty = 2,  bty = "n")

ix <- which((selectedDraws[, "C1"] <= 0) & (selectedDraws[, "C2"] <= 0))
plot(
  selectedDraws[, "C1"],
  selectedDraws[, "C2"],
  cex = 0.2,
  cex.axis = 1,
  xlab = expression(C[1]),
  ylab = expression(C[2])
)
lines(c(-5, 0), c(0, 0), lty = 2, lwd = 1)
lines(c(0, 0), c(-2, 0), lty = 2, lwd = 1)
points(selectedDraws[, "C1"][ix], selectedDraws[, "C2"][ix], col = "red",
       cex = 0.2, pch = 19)
```

### Highest Posterior Density (HPD) intensity plots
```{r, fig.height = 8, fig.width = 9, fig.cap = "HPD plots"}
par(mfcol = c(2, 2))
ttIdx <- 0
for (tt in kTransitionTypes) {
  ttIdx <- ttIdx + 1
  PlotHPDOverTime(
    intensityDraws[, , ttIdx] * max(transitionData$nummonth),
    transitions[, tt],
    dates,
    ylab = "Intensity",
    statStr = "Intensity",
    obsStr = paste(tt, "counts"),
    modeCol = rgb(0,0,1,1), 
    fillCol = rgb(0,0,1,1/4),
    borderCol = rgb(0,0,1,1/2),
    newPlot = FALSE,
    ylim = NULL,
    dateAxisStep = "1 year"
  )
}
```
